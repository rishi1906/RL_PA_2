{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Installing packages for rendering the game on Colab\n",
        "'''\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "zve95x_CZIk-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "918f63b5-6d25-4960-efc3-5e6e1b6c07ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.5/821.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-69.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "24dc133ae08d458abbd632b546c00805"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "# Replace with your actual API key\n",
        "api_key = \"8f58df9a66485e9ea9149b8b599cb14eb71832dc\"\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=api_key)"
      ],
      "metadata": {
        "id": "LjcjlGuQZVfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3deac021-bd8f-410a-ddfa-481d4602d718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp\n",
        "import wandb"
      ],
      "metadata": {
        "id": "2anBcjTiil7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "R5KV6KnciwII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "'''\n",
        "List of example environments\n",
        "(Source - https://gym.openai.com/envs/#classic_control)\n",
        "'Acrobot-v1'\n",
        "'Cartpole-v1'\n",
        "\n",
        "'''\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "# print(state_shape)\n",
        "# print(no_of_actions)\n",
        "# print(env.action_space.sample())\n",
        "# print(\"----\")\n",
        "\n",
        "'''\n",
        "# Understanding State, Action, Reward Dynamics\n",
        "\n",
        "The agent decides an action to take depending on the state.\n",
        "\n",
        "The Environment keeps a variable specifically for the current state.\n",
        "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "- It returns the new current state and reward for the agent to take the next action\n",
        "\n",
        "'''\n",
        "\n",
        "state = env.reset()\n",
        "''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "# print(state)\n",
        "# print(\"----\")\n",
        "\n",
        "action = env.action_space.sample()\n",
        "''' We take a random action now '''\n",
        "\n",
        "# print(action)\n",
        "# print(\"----\")\n",
        "\n",
        "next_state, reward, done, info = env.step(action)\n",
        "''' env.step is used to calculate new state and obtain reward based on old state and action taken  '''\n",
        "\n",
        "# print(next_state)\n",
        "# print(reward)\n",
        "# print(done)\n",
        "# print(info)\n",
        "# print(\"----\")\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "state_size = state_shape\n"
      ],
      "metadata": {
        "id": "OVn2Ox8Hi5af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Dueling DQN model\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, fc1_units, fc2_units, seed):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(input_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.advantage = nn.Linear(fc2_units, output_size)\n",
        "        self.value = nn.Linear(fc2_units, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        advantage = self.advantage(x)\n",
        "        value = self.value(x)\n",
        "        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
        "        return q_values\n"
      ],
      "metadata": {
        "id": "5SXQlNoci9Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ReplayBuffer class\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "BfzM7c3TjDxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the agent\n",
        "class Agent():\n",
        "    def __init__(self, state_size, action_size, fc1_units, fc2_units, buffer_size, batch_size, lr, update_every, gamma, eps_end, eps_decay, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.update_every = update_every\n",
        "        self.gamma = gamma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.q_network = DuelingDQN(state_size, action_size, fc1_units, fc2_units, seed).to(device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, fc1_units, fc2_units, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
        "        self.t_step = 0\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) >= self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.0):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.q_network.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.q_network(state)\n",
        "        self.q_network.train()\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        Q_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        wandb.log({'train loss': loss})\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)  # Gradient clipping\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "9TzHGE1Ei_j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the DQN algorithm\n",
        "# def dqn(agent, env, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "#     scores_window = deque(maxlen=100)\n",
        "#     episode_list_epsgrdy = []\n",
        "#     average_scores_epsgrdy = []\n",
        "#     eps = eps_start\n",
        "\n",
        "#     for i_episode in range(1, n_episodes+1):\n",
        "#         state = env.reset()\n",
        "#         score = 0\n",
        "\n",
        "#         for t in range(max_t):\n",
        "#             action = agent.act(state, eps)\n",
        "#             next_state, reward, done, _ = env.step(action)\n",
        "#             agent.step(state, action, reward, next_state, done)\n",
        "#             state = next_state\n",
        "#             score += reward\n",
        "\n",
        "#             if done:\n",
        "#                 break\n",
        "\n",
        "#         scores_window.append(score)\n",
        "#         average_score = np.mean(scores_window)\n",
        "#         episode_list_epsgrdy.append(i_episode)\n",
        "#         average_scores_epsgrdy.append(average_score)\n",
        "#         eps = max(eps_end, eps_decay*eps)\n",
        "#         print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
        "\n",
        "#         if i_episode % 100 == 0:\n",
        "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
        "\n",
        "#         if average_score >= 195.0:\n",
        "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
        "#             break\n",
        "\n",
        "\n",
        "\n",
        "#     return episode_list_epsgrdy, average_scores_epsgrdy\n"
      ],
      "metadata": {
        "id": "6Nzd5bzOjG0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn(agent, env, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    episode_list_epsgrdy = []\n",
        "    average_scores_epsgrdy = []\n",
        "    cumulative_regret = 0  # Initialize cumulative regret\n",
        "    eps = eps_start\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        regret = 0  # Initialize regret for this episode\n",
        "\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            optimal_action = np.argmax(agent.q_network(torch.from_numpy(state).float().unsqueeze(0)).cpu().data.numpy())\n",
        "            optimal_reward = env.step(optimal_action)[1]  # Get the reward for the optimal action\n",
        "            regret += optimal_reward - reward  # Calculate regret for this time step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        cumulative_regret += regret  # Update cumulative regret\n",
        "        scores_window.append(score)\n",
        "        average_score = np.mean(scores_window)\n",
        "        episode_list_epsgrdy.append(i_episode)\n",
        "        average_scores_epsgrdy.append(average_score)\n",
        "        wandb.log({'average_score': average_scores_epsgrdy})\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
        "\n",
        "        if average_score >= 195.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
        "            break\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "\n",
        "    return episode_list_epsgrdy, average_scores_epsgrdy, cumulative_regret"
      ],
      "metadata": {
        "id": "oqeWaJOf7u4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize wandb with your project name\n",
        "wandb.init(project=\"DeulingDQN\")"
      ],
      "metadata": {
        "id": "EPQxglT2j1Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlH6wcN3YqU4"
      },
      "outputs": [],
      "source": [
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\"goal\": \"minimize\", \"name\": \"cumulative_regret\"},\n",
        "    \"parameters\": {\n",
        "        'state_size': {\n",
        "            'values': [4]\n",
        "        },\n",
        "        'action_size': {\n",
        "            'values': [2]\n",
        "        },\n",
        "        'BUFFER_SIZE': {\n",
        "            # 'values': [int(1e5)]\n",
        "            'values': [int(1e4), int(1e5), int(1e6)]\n",
        "        },\n",
        "        'BATCH_SIZE': {\n",
        "            # 'values': [64]\n",
        "            'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'LR': {\n",
        "            # 'values': [5e-4]\n",
        "            'values': [0.1, 0.01, 0.001, 0.0001]\n",
        "        },\n",
        "        'UPDATE_EVERY': {\n",
        "            # 'values': [20]\n",
        "            'values': [4, 6, 10, 20]\n",
        "        },\n",
        "        'fc1_units': {\n",
        "            # 'values': [128]\n",
        "            'values': [64, 128, 256]\n",
        "        },\n",
        "        'fc2_units': {\n",
        "            # 'values': [64]\n",
        "            'values': [64, 128, 256]\n",
        "        },\n",
        "        'eps_start': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'eps_end': {\n",
        "            # 'values': [0.01]\n",
        "            'values': [0.01, 0.05, 0.1]\n",
        "        },\n",
        "        'eps_decay': {\n",
        "            # 'values': [0.995]\n",
        "            'values': [0.9, 0.95, 0.99, 0.995, 0.999]\n",
        "        },\n",
        "        'gamma': {\n",
        "            'values': [0.99]\n",
        "        },\n",
        "        'n_episodes': {\n",
        "            # 'values': [10000]\n",
        "            'values': [1000, 2000, 5000]\n",
        "        },\n",
        "        'max_t': {\n",
        "            # 'values': [1000]\n",
        "            'values': [500, 1000, 2000]\n",
        "        },\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='DeulingDQN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the main function for hyperparameter tuning\n",
        "def main():\n",
        "\n",
        "    with wandb.init() as run:\n",
        "        # Get the hyperparameters for this run\n",
        "        config = wandb.config\n",
        "\n",
        "        begin_time = datetime.datetime.now()\n",
        "\n",
        "\n",
        "\n",
        "        # Create the agent with the hyperparameters\n",
        "        agent = Agent(state_size=config.state_size, action_size=config.action_size, fc1_units=config.fc1_units, fc2_units=config.fc2_units, buffer_size=config.BUFFER_SIZE, batch_size=config.BATCH_SIZE, lr=config.LR, update_every=config.UPDATE_EVERY, gamma=config.gamma, eps_end=config.eps_end, eps_decay=config.eps_decay,seed = 0)\n",
        "\n",
        "        # Train the agent and get the scores\n",
        "        # episode_list_epsgrdy, average_scores_epsgrdy = dqn(agent, env, n_episodes=config.n_episodes, max_t=config.max_t, eps_start=config.eps_start, eps_end=config.eps_end, eps_decay=config.eps_decay)\n",
        "\n",
        "        episode_list_epsgrdy, average_scores_epsgrdy, cumulative_regret = dqn(agent, env, n_episodes=config.n_episodes, max_t=config.max_t, eps_start=config.eps_start, eps_end=config.eps_end, eps_decay=config.eps_decay)\n",
        "\n",
        "        time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "        print(time_taken)\n",
        "        # Log the final average score to wandb\n",
        "        wandb.log({\"Average Score\": average_scores_epsgrdy})\n",
        "        wandb.log({\"Average Score\": cumulative_regret})\n",
        "\n"
      ],
      "metadata": {
        "id": "-fCv_1iLcRRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Run the sweep\n",
        "wandb.agent(sweep_id, function=main, count=10)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "lY2BLSyWjLuE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}