{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "# Tutorial 8 - Options\n",
        "\n",
        "Please complete this tutorial to get an overview of options and an implementation of SMDP Q-Learning and Intra-Option Q-Learning.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        " [Recent Advances in Hierarchical Reinforcement\n",
        "Learning](https://people.cs.umass.edu/~mahadeva/papers/hrl.pdf) is a strong recommendation for topics in HRL that was covered in class. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade gym"
      ],
      "metadata": {
        "id": "kzx_p4JlWY3U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gym\n",
        "#from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "3e2cbbf7-c6b6-47ed-9bf8-22267924e71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# '''\n",
        "# The environment used here is extremely similar to the openai gym ones.\n",
        "# At first glance it might look slightly different.\n",
        "# The usual commands we use for our experiments are added to this cell to aid you\n",
        "# work using this environment.\n",
        "# '''\n",
        "\n",
        "# #Setting up the environment\n",
        "# from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "# env = CliffWalkingEnv()\n",
        "\n",
        "# env.reset()\n",
        "\n",
        "# #Current State\n",
        "# print(env.s)\n",
        "\n",
        "# # 4x12 grid = 48 states\n",
        "# print (\"Number of states:\", env.nS)\n",
        "\n",
        "# # Primitive Actions\n",
        "# action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "# #correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# # either go left, up, down or right\n",
        "# print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# # Example Transitions\n",
        "# rnd_action = random.randint(0, 3)\n",
        "# print (\"Action taken:\", action[rnd_action])\n",
        "# next_state, reward, is_terminal, t_prob, *extras = env.step(rnd_action)\n",
        "# print (\"Transition probability:\", t_prob)\n",
        "# print (\"Next state:\", next_state)\n",
        "# print (\"Reward recieved:\", reward)\n",
        "# print (\"Terminal state:\", is_terminal)\n",
        "# env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the environment with rendering mode\n",
        "from gym.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
        "env = CliffWalkingEnv(render_mode='ansi')  # Specify the rendering mode here\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Current State\n",
        "print(env.s)\n",
        "\n",
        "# 4x12 grid = 48 states\n",
        "print(\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"up\", \"right\", \"down\", \"left\"]\n",
        "# Correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# Either go left, up, down or right\n",
        "print(\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 3)\n",
        "print(\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob, *extras = env.step(rnd_action)\n",
        "print(\"Transition probability:\", t_prob)\n",
        "print(\"Next state:\", next_state)\n",
        "print(\"Reward received:\", reward)\n",
        "print(\"Terminal state:\", is_terminal)\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwGR-kntWxcH",
        "outputId": "2ad3aa7c-d226-431b-ebff-e398bf59dd35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "Number of states: 48\n",
            "Number of actions that an agent can take: 4\n",
            "Action taken: left\n",
            "Transition probability: False\n",
            "Next state: 36\n",
            "Reward received: -1\n",
            "Terminal state: False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o  o  o  o  o  o  o  o  o  o  o  o\\no  o  o  o  o  o  o  o  o  o  o  o\\no  o  o  o  o  o  o  o  o  o  o  o\\nx  C  C  C  C  C  C  C  C  C  C  T\\n\\n',\n",
              " 'o  o  o  o  o  o  o  o  o  o  o  o\\no  o  o  o  o  o  o  o  o  o  o  o\\no  o  o  o  o  o  o  o  o  o  o  o\\nx  C  C  C  C  C  C  C  C  C  C  T\\n\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "me0GWEW2X8Ya"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "#### Options\n",
        "We custom define very simple options here. They might not be the logical options for this settings deliberately chosen to visualise the Q Table better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g4MRC1p2DZbp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bcd85213-9c89-43cc-f201-5a40a296244d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNow the new action space will contain\\nPrimitive Actions: [\"up\", \"right\", \"down\", \"left\"]\\nOptions: [\"Away\",\"Close\"]\\nTotal Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\\nCorresponding to [0,1,2,3,4,5]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# We are defining two more options here\n",
        "# Option 1 [\"Away\"] - > Away from Cliff (ie keep going up)\n",
        "# Option 2 [\"Close\"] - > Close to Cliff (ie keep going down)\n",
        "\n",
        "def Away(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 0\n",
        "\n",
        "    if (int(state/12) == 0):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "def Close(env,state):\n",
        "\n",
        "    optdone = False\n",
        "    optact = 2\n",
        "\n",
        "    if (int(state/12) == 2):\n",
        "        optdone = True\n",
        "\n",
        "    if (int(state/12) == 3):\n",
        "        optdone = True\n",
        "\n",
        "    return [optact,optdone]\n",
        "\n",
        "\n",
        "'''\n",
        "Now the new action space will contain\n",
        "Primitive Actions: [\"up\", \"right\", \"down\", \"left\"]\n",
        "Options: [\"Away\",\"Close\"]\n",
        "Total Actions :[\"up\", \"right\", \"down\", \"left\", \"Away\", \"Close\"]\n",
        "Corresponding to [0,1,2,3,4,5]\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "# Task 1\n",
        "Complete the code cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "#Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "q_values_SMDP = np.zeros((48,6))\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "option_freq_SMDP = np.zeros(q_values_SMDP.shape)\n",
        "# TODO: epsilon-greedy action selection function\n",
        "def egreedy_policy(q_values, state, epsilon):\n",
        "    if np.random.rand() >= epsilon:\n",
        "        # Choose the action with the highest Q-value\n",
        "        return np.argmax(q_values[state])\n",
        "    else:\n",
        "        # Choose a random action\n",
        "        return np.random.randint(q_values.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "# Task 2\n",
        "Below is an incomplete code cell with the flow of SMDP Q-Learning. Complete the cell and train the agent using SMDP Q-Learning algorithm.\n",
        "Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ok_5eQM7OCTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ceea4b5-e3e3-49c9-f969-97b5e1914534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPISODE: 999"
          ]
        }
      ],
      "source": [
        "#### SMDP Q-Learning\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(f\"\\rEPISODE: {_}\", end = \"\")\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "        _state = state\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon=0.1)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            # Perform regular Q-Learning update for state-action pair\n",
        "            next_state, reward, done, _,a = env.step(action)\n",
        "            q_values_SMDP[state, action] += alpha * (reward + gamma * np.max(q_values_SMDP[next_state]) - q_values_SMDP[state, action])\n",
        "            state = next_state\n",
        "            option_freq_SMDP[state][action] += 1\n",
        "\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        reward_bar = 0\n",
        "        tau = 0\n",
        "        if action == 4: # action => Away option\n",
        "\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "\n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Away(env,state)\n",
        "                next_state, reward, done,_,a = env.step(optact)\n",
        "\n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                if optdone or done:\n",
        "                    q_values_SMDP[_state, action] += alpha * (reward_bar + (gamma**tau) * np.max(q_values_SMDP[next_state]) - q_values_SMDP[_state,action])\n",
        "                    option_freq_SMDP[_state][action] += 1\n",
        "                    state = next_state\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "        if action == 5: # action => Close option\\\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "                # Think about what this function might do?\n",
        "                optact ,optdone = Close(env,state)\n",
        "                next_state, reward, opt,_,a = env.step(optact)\n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "                # Complete SMDP Q-Learning Update\n",
        "                # Remember SMDP Updates. When & What do you update?\n",
        "                if optdone:\n",
        "                    q_values_SMDP[_state, action] += alpha * (reward_bar +(gamma**tau) * np.max(q_values_SMDP[next_state]) - q_values_SMDP[_state,action])\n",
        "                    option_freq_SMDP[_state][action] += 1\n",
        "                state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "# Task 3\n",
        "Using the same options and the SMDP code, implement Intra Option Q-Learning (In the code cell below). You *might not* always have to search through options to find the options with similar policies, think about it. Keep the **final Q-table** and **Update Frequency** table handy (You'll need it in TODO 4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Q-Table: (States x Actions) === (env.ns(48) x total actions(6))\n",
        "q_values_IO = np.zeros((48,6))\n",
        "#Update_Frequency Data structure? Check TODO 4\n",
        "option_freq_IO = np.zeros(q_values_SMDP.shape)"
      ],
      "metadata": {
        "id": "-gaEr4Tie306"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r6A2TdUHWVUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67666ed1-8647-4491-c51c-7bd424abcfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPISODE: 999"
          ]
        }
      ],
      "source": [
        "#### Intra-Option Q-Learning\n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.99\n",
        "alpha = 0.1\n",
        "epsilon =0.1\n",
        "\n",
        "# Iterate over 1000 episodes\n",
        "for _ in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(f\"\\rEPISODE: {_}\", end = \"\")\n",
        "    # While episode is not over\n",
        "    while not done:\n",
        "\n",
        "\n",
        "        # Choose action\n",
        "        action = egreedy_policy(q_values_SMDP, state, epsilon)\n",
        "\n",
        "        # Checking if primitive action\n",
        "        if action < 4:\n",
        "          # Perform regular Q-Learning update for state-action pair\n",
        "          next_state, reward, done, _,a = env.step(action)\n",
        "          q_values_IO[state, action] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, action])\n",
        "          state = next_state\n",
        "          option_freq_IO[state][action] += 1\n",
        "\n",
        "\n",
        "        # Checking if action chosen is an option\n",
        "        if action == 4: # action => Away option\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Away(env,state)\n",
        "                next_state, reward, done,_,a = env.step(optact)\n",
        "\n",
        "                # Complete IO Q Learning Update\n",
        "                q_values_IO[state, optact] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, optact])\n",
        "                option_freq_IO[state, optact] += 1\n",
        "\n",
        "                # options qvalues\n",
        "                if optdone:\n",
        "                    q_values_IO[state, action] += alpha * (reward + gamma * (np.max(q_values_IO[next_state])) - q_values_IO[state, action] )\n",
        "                else:\n",
        "                    q_values_IO[state, action] += alpha * (reward + gamma *(q_values_IO[next_state, action]) - q_values_IO[state, action] )\n",
        "\n",
        "\n",
        "\n",
        "                option_freq_IO[state, action] += 1\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "\n",
        "        if action == 5: # action => Close option\\\n",
        "            optdone = False\n",
        "            while (optdone == False):\n",
        "                # Think about what this function might do?\n",
        "                optact,optdone = Close(env,state)\n",
        "                next_state, reward, done,_,a = env.step(optact)\n",
        "                # Is this formulation right? What is this term?\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                tau += 1\n",
        "                q_values_IO[state, optact] += alpha * (reward + gamma * np.max(q_values_IO[next_state]) - q_values_IO[state, optact])\n",
        "                option_freq_IO[state][optact] += 1\n",
        "\n",
        "                 #options q_values\n",
        "\n",
        "                if optdone:\n",
        "                    q_values_IO[state, action] += alpha * (reward + gamma * (np.max(q_values_IO[next_state])) - q_values_IO[state, action] )\n",
        "                else:\n",
        "                    q_values_IO[state, action] += alpha * (reward + gamma *(q_values_IO[next_state, action]) - q_values_IO[state, action] )\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzUgcwL-VfkO"
      },
      "source": [
        "# Task 4\n",
        "Compare the two Q-Tables and Update Frequencies and provide comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "v8mZE74_Vhmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96908b8-ce44-471b-a985-3875816036be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999974,   -9.99999999],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.8999995 , -108.99999995],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999569, -108.99999988],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999983, -108.99999995],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999992, -108.99999991],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999975, -108.99999994],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999986, -108.99999978],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999987, -108.99999995],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999982, -108.99999913],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -10.89999794, -108.99999982],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998,   -9.99999998,\n",
              "         -10.89999896, -108.99999937],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998,   -9.99999998,\n",
              "         -10.89999931,   -9.99999998],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.5389995 ,   -9.99999999],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899887, -108.99999993],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899965, -108.99999947],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899987, -108.99999956],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899956, -108.99999984],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899976, -108.99999989],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899982, -108.99999887],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899977, -108.99999995],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899961, -108.99999973],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.53899984, -108.99999292],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998,   -9.99999998,\n",
              "         -11.53899847, -108.99999768],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998,   -9.99999998,\n",
              "         -11.53895011,   -9.99999998],\n",
              "       [  -9.99999999,   -9.99999999,   -9.99999999,   -9.99999999,\n",
              "         -11.97558999,   -9.99999999],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999998,   -9.99999999,\n",
              "         -11.97558997, -108.99999986],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999928,   -9.99999999,\n",
              "         -11.97558988, -108.99999575],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999541,   -9.99999999,\n",
              "         -11.97558989, -108.99999982],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999985,   -9.99999999,\n",
              "         -11.97558981, -108.99999558],\n",
              "       [  -9.99999999,   -9.99999999, -108.99986588,   -9.99999999,\n",
              "         -11.97558921, -108.99999811],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999947,   -9.99999999,\n",
              "         -11.97557696, -108.99999128],\n",
              "       [  -9.99999999,   -9.99999999, -108.99994257,   -9.99999999,\n",
              "         -11.97557599, -108.9999764 ],\n",
              "       [  -9.99999999,   -9.99999999, -108.99999773,   -9.99999999,\n",
              "         -11.97558926, -108.99991485],\n",
              "       [  -9.99999999,   -9.99999999, -108.99994248,   -9.99999999,\n",
              "         -11.97558972, -108.9999994 ],\n",
              "       [  -9.99999998,   -9.99999998, -108.99971533,   -9.99999998,\n",
              "         -11.97558943, -108.99999736],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998,   -9.99999998,\n",
              "         -11.97558982,   -9.99999998],\n",
              "       [  -9.99999999, -108.99999999,   -9.99999999,   -9.99999999,\n",
              "         -12.25632789,   -9.99999999],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [  -9.99999998,   -9.99999998,   -9.99999998, -108.99963714,\n",
              "         -12.25627029,   -9.99999998]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "q_values_SMDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SemE13ORV04n"
      },
      "source": [
        "Use this text cell for your comments - Task 4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_values_IO"
      ],
      "metadata": {
        "id": "aMKdANGIjEVB",
        "outputId": "b12f432b-6f02-46e7-b2b4-ceaa4f5cff2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  -3.15966547,   -3.8108354 ,   -2.18148102,   -3.15803174,\n",
              "          -3.15964612,   -6.0174502 ],\n",
              "       [  -3.83894237,   -3.68764781,   -2.86761859,   -3.15953706,\n",
              "          -3.82622423, -103.05716065],\n",
              "       [  -3.67371278,   -4.64906761,   -2.77210764,   -3.04821188,\n",
              "          -3.67366402,  -80.26697171],\n",
              "       [  -4.65007594,   -5.59188778,   -4.12588661,   -3.69185836,\n",
              "          -4.64731518, -103.06551979],\n",
              "       [  -5.60457481,   -4.99225139,   -4.9748281 ,   -4.65404957,\n",
              "          -5.60406491, -103.06537724],\n",
              "       [  -5.02275338,   -4.07105245,   -4.07252156,   -5.60335347,\n",
              "          -5.01755161, -103.06569798],\n",
              "       [  -4.06382949,   -3.10207384,   -3.11260132,   -5.02953898,\n",
              "          -4.05828491, -103.06568474],\n",
              "       [  -3.12278193,   -2.14662614,   -2.47990669,   -3.99484429,\n",
              "          -3.10032758, -103.01673621],\n",
              "       [  -2.1283847 ,   -2.19491196,   -1.20846008,   -1.51515327,\n",
              "          -2.12837194,  -13.90701779],\n",
              "       [  -1.86360406,   -1.62022821,   -2.1821479 ,   -1.21042038,\n",
              "          -1.83653454,  -56.27495122],\n",
              "       [  -2.27089356,   -2.95711533,   -1.28373122,   -2.14953569,\n",
              "          -2.27009578, -102.62511113],\n",
              "       [  -2.99131733,   -2.99081798,   -2.01143168,   -2.27088982,\n",
              "          -2.99131434,   -2.9701    ],\n",
              "       [  -3.15937016,   -2.85906742,   -3.15796099,   -1.19342195,\n",
              "          -4.12728286,   -5.08102154],\n",
              "       [  -3.83606612,   -2.56799313,   -3.79721362,   -1.88783775,\n",
              "          -4.7023324 , -103.09488694],\n",
              "       [  -3.66735431,   -4.10636953,   -3.79008084,   -2.1981171 ,\n",
              "          -4.58328336, -100.71455524],\n",
              "       [  -4.64771324,   -5.00864253,   -4.91912388,   -3.15746122,\n",
              "          -5.58662256, -103.09677322],\n",
              "       [  -5.60750249,   -4.05056988,   -5.6631083 ,   -4.10353257,\n",
              "          -6.5430442 , -103.09678211],\n",
              "       [  -5.01236463,   -3.12676899,   -4.70162859,   -4.38539998,\n",
              "          -5.94951143, -103.09677163],\n",
              "       [  -4.06601719,   -2.15585946,   -3.75644252,   -3.762315  ,\n",
              "          -4.85473771, -103.09676004],\n",
              "       [  -3.07722198,   -2.17308221,   -3.25470775,   -1.58092263,\n",
              "          -4.01667913, -103.07623767],\n",
              "       [  -2.12324768,   -2.18605568,   -2.60970176,   -1.36829137,\n",
              "          -3.03054074,  -76.12147454],\n",
              "       [  -1.96729088,   -1.20535472,   -2.99446682,   -1.70970716,\n",
              "          -2.43095338,  -97.05293398],\n",
              "       [  -2.08513127,   -0.28660397,   -2.19924099,   -2.15855992,\n",
              "          -2.88643686, -102.88405399],\n",
              "       [  -2.99131709,   -1.51941468,   -1.99      ,   -1.02164825,\n",
              "          -3.96068562,   -1.99      ],\n",
              "       [  -2.18148416,   -3.82202301,   -4.12797952,   -3.14150467,\n",
              "          -5.0826751 ,   -4.12770884],\n",
              "       [  -2.85255759,   -4.11476344, -103.12805751,   -3.14973901,\n",
              "          -5.54250415, -103.1279971 ],\n",
              "       [  -3.1489194 ,   -4.92752245, -103.12745307,   -3.71915012,\n",
              "          -5.4819941 , -103.11904728],\n",
              "       [  -4.06502351,   -5.66982677, -103.12807007,   -3.96720017,\n",
              "          -6.47743778, -103.12806889],\n",
              "       [  -4.9903179 ,   -4.71760642, -103.12807206,   -4.91162304,\n",
              "          -7.47185034, -103.1280714 ],\n",
              "       [  -4.0753251 ,   -3.75554934, -103.12807172,   -5.67039797,\n",
              "          -6.86635481, -103.12807078],\n",
              "       [  -2.78686994,   -3.44920513, -103.12806697,   -3.73635622,\n",
              "          -4.94235044, -103.12806665],\n",
              "       [  -2.50171336,   -3.08767111, -103.12771547,   -3.64372048,\n",
              "          -4.89240955, -103.12614422],\n",
              "       [  -2.12147942,   -2.94629281, -103.12405465,   -3.47534206,\n",
              "          -3.94701453, -103.11156276],\n",
              "       [  -2.02317465,   -2.22928274, -103.08135908,   -2.33324282,\n",
              "          -2.73154546, -102.98295579],\n",
              "       [  -1.24343876,   -1.99      , -103.08966119,   -2.55818252,\n",
              "          -2.91807348, -103.06341024],\n",
              "       [  -2.01143074,   -1.99      ,   -1.        ,   -2.22567608,\n",
              "          -4.9177673 ,   -1.        ],\n",
              "       [  -3.15966931, -103.12177851,   -4.12648067,   -4.11705127,\n",
              "          -6.01678805,   -4.11817675],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ],\n",
              "       [   0.        ,    0.        ,    0.        ,    0.        ,\n",
              "           0.        ,    0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(option_freq_SMDP),np.sum(option_freq_IO))"
      ],
      "metadata": {
        "id": "vLON4mUgjFGe",
        "outputId": "c1150f7c-dd9a-44cc-ebf5-55d0f7d33a9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168762.0 832948.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTHmlk-XjNlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}