{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "# Replace with your actual API key\n",
        "api_key = \"17dab9d1bbdc37c41831799a4b0b50d3e97400c5\"\n",
        "project_name='RBCS1'\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlW33Sjz2WUd",
        "outputId": "e174e242-acd1-4789-e670-be846c20bcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.44.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.44.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "torch.manual_seed(0)\n",
        "import base64\n",
        "import io  # For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "import datetime"
      ],
      "metadata": {
        "id": "r9eE0OjXWWW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff9daf5-307c-4c03-d848-b8d56500e1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)"
      ],
      "metadata": {
        "id": "wJTy0fUIWY_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "# print('observation space:', env.observation_space)\n",
        "# print('action space:', env.action_space)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hse5VDflWeUk",
        "outputId": "cafd7d64-4a41-4d56-dd3d-5c70fbdc4c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        # we just consider 1 dimensional probability of action\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        if state.shape[1] != 4:  # Check if the state tensor has the expected shape\n",
        "            state = state.view(1, 4)  # Reshape the state tensor to (1, 4)\n",
        "        probs = self.forward(state).cpu()\n",
        "        model = Categorical(probs)\n",
        "        action = model.sample()\n",
        "        return action.item(), model.log_prob(action)"
      ],
      "metadata": {
        "id": "R2uE3MgCNh5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size=32):\n",
        "        super(Baseline, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dRKyEts2NlN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1B1nA1J0Fxa"
      },
      "outputs": [],
      "source": [
        "def reinforce(env, policy, optimizer, baseline, baseline_optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for e in range(1, n_episodes):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        states = []\n",
        "        state = env.reset()\n",
        "        for t in range(max_t):\n",
        "            states.append(state)\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
        "        R = sum([a * b for a, b in zip(discounts, rewards)])\n",
        "\n",
        "        # Calculate the baseline\n",
        "        baseline_values = []\n",
        "        for state_in_trajectory in states:\n",
        "            state_tensor = torch.from_numpy(state_in_trajectory).float().unsqueeze(0).to(device)\n",
        "            baseline_value = baseline(state_tensor)\n",
        "            baseline_values.append(baseline_value)\n",
        "        baseline_values = torch.cat(baseline_values, dim=0).requires_grad_()\n",
        "\n",
        "        # Calculate the policy loss\n",
        "        policy_loss = []\n",
        "        for log_prob, baseline_value in zip(saved_log_probs, baseline_values):\n",
        "            policy_loss.append(-(log_prob * (R - baseline_value)))\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Update the policy\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward(retain_graph=True)  # Retain the computation graph\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the baseline\n",
        "        baseline_optimizer.zero_grad()\n",
        "        baseline_loss = ((baseline_values - R) ** 2).mean()\n",
        "        baseline_loss.backward()\n",
        "        baseline_optimizer.step()\n",
        "\n",
        "        if e % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
        "        if np.mean(scores_deque) >= 195.0:\n",
        "            print(f'Environment {env.unwrapped.spec.id} solved in {e - 100:d} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
        "            break\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# policy = Policy().to(device)\n",
        "# optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "# baseline = Baseline(state_size=env.observation_space.shape[0]).to(device)\n",
        "# baseline_optimizer = optim.Adam(baseline.parameters(), lr=1e-2)\n",
        "# scores = reinforce(env, policy, optimizer, baseline, baseline_optimizer, n_episodes=2000)\n",
        "# wandb.finish()  # Finish wandb run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=project_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "KxvEp_SBuD2-",
        "outputId": "e6fe51f9-4b5c-4db0-93c2-3731d8132d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishi1906\u001b[0m (\u001b[33miitm_aero\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240409_045916-bc2lm45e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/iitm_aero/RBCS1/runs/bc2lm45e' target=\"_blank\">honest-breeze-1</a></strong> to <a href='https://wandb.ai/iitm_aero/RBCS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/iitm_aero/RBCS1' target=\"_blank\">https://wandb.ai/iitm_aero/RBCS1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/iitm_aero/RBCS1/runs/bc2lm45e' target=\"_blank\">https://wandb.ai/iitm_aero/RBCS1/runs/bc2lm45e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/iitm_aero/RBCS1/runs/bc2lm45e?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f77c068d9c0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project=\"R_CartPole_baseline\", config={  # Initialize wandb first\n",
        "#     \"hidden_size\": 32,  # Default values\n",
        "#     \"n_episodes\": 1000,\n",
        "#     \"max_t\": 1000,\n",
        "#     \"gamma\": 0.99,\n",
        "#     \"print_every\": 100,\n",
        "#     \"lr\": 1e-2,\n",
        "# })\n",
        "\n",
        "# config = {\n",
        "#     \"hidden_size\": wandb.config.hidden_size,  # Access wandb.config after initialization\n",
        "#     \"n_episodes\": wandb.config.n_episodes,\n",
        "#     \"max_t\": wandb.config.max_t,\n",
        "#     \"gamma\": wandb.config.gamma,\n",
        "#     \"print_every\": wandb.config.print_every,\n",
        "#     \"lr\": wandb.config.lr,\n",
        "# }\n"
      ],
      "metadata": {
        "id": "w1C4OKK9NTi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(env.spec.reward_threshold)"
      ],
      "metadata": {
        "id": "Y4j7y2t8PU5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-BD5W3QN8UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "      'name': 'Average Reward',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'state_size': {\n",
        "            'values': [state_shape]\n",
        "        },\n",
        "        'action_size': {\n",
        "            'values': [action_shape]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [32,64,128]\n",
        "        },\n",
        "        'max_t': {\n",
        "            'values': [500, 1000]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-2, 1e-3]\n",
        "        },\n",
        "        'baseline_lr': {\n",
        "            'values': [1e-2, 1e-3]\n",
        "        },\n",
        "        'gamma': {\n",
        "            'values': [0.99]\n",
        "        },\n",
        "        'alpha': {\n",
        "            'values': [0.001, 0.0001]\n",
        "        },\n",
        "        'n_episodes': {\n",
        "            'values': [2000]\n",
        "        },\n",
        "         \"print_every\": {\n",
        "             'values': [100]\n",
        "         }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "odkefnoFuLrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sweep_config = {\n",
        "#     'method': 'grid',\n",
        "#     'metric': {\n",
        "#       'name': 'Average Reward',\n",
        "#       'goal': 'maximize'\n",
        "#     },\n",
        "#     'parameters': {\n",
        "#         'hidden_size': {\n",
        "#             'values': [32,64,128]\n",
        "#         },\n",
        "#         'max_t': {\n",
        "#             'values': [500, 1000, 1500]\n",
        "#         },\n",
        "#         'lr': {\n",
        "#             'values': [1e-5, 1e-3,1e-4]\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ],
      "metadata": {
        "id": "Sn8tdcXc1Gte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new sweep\n",
        "sweep_id = wandb.sweep(sweep_config,  project=project_name)\n",
        "max_sweep_run = 10 #update it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QMZhG_F0TZB",
        "outputId": "ba6f3809-88c2-4539-cb6f-a88d137acd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: h6mmiigu\n",
            "Sweep URL: https://wandb.ai/iitm_aero/RBCS1/sweeps/h6mmiigu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(config=None):\n",
        "#     # Set default values for hyperparameters\n",
        "#     default_config = {\n",
        "#         \"hidden_size\": 32,\n",
        "#         \"n_episodes\": 1000,\n",
        "#         \"max_t\": 1000,\n",
        "#         \"gamma\": 1.0,\n",
        "#         \"print_every\": 100,\n",
        "#         \"lr\": 1e-2,\n",
        "#         \"baseline_lr\": 1e-2,\n",
        "#     }\n",
        "\n",
        "#     # Initialize a new wandb run\n",
        "#     run = wandb.init(config=config, reinit=True)\n",
        "\n",
        "#     # If config is None, use the default values\n",
        "#     if config is None:\n",
        "#         run.config.update(default_config, allow_val_change=True)\n",
        "#     config = run.config\n",
        "\n",
        "#     # Initialize the environment and seed\n",
        "#     env = gym.make('CartPole-v1')\n",
        "#     env.seed(0)\n",
        "\n",
        "#     # Create the policy network with the specified hyperparameters\n",
        "#     policy = Policy(state_size=4, action_size=2, hidden_size=config.hidden_size).to(device)\n",
        "#     optimizer = optim.Adam(policy.parameters(), lr=config.lr)\n",
        "\n",
        "#     # Create the baseline network with the specified hyperparameters\n",
        "#     baseline = Baseline(state_size=4).to(device)\n",
        "#     baseline_optimizer = optim.Adam(baseline.parameters(), lr=config.baseline_lr)\n",
        "\n",
        "#     # Run the REINFORCE algorithm with the specified hyperparameters\n",
        "#     scores = reinforce(env, policy, optimizer, baseline, baseline_optimizer, n_episodes=config.n_episodes, max_t=config.max_t, gamma=config.gamma, print_every=config.print_every)\n",
        "\n",
        "#     # Log the final score as a summary metric\n",
        "#     run.summary[\"final_score\"] = np.mean(scores[-100:])\n",
        "\n",
        "#     # Finish the wandb run\n",
        "#     run.finish()\n",
        "\n",
        "#     # Return any necessary values or metrics\n",
        "#     return np.mean(scores[-100:])"
      ],
      "metadata": {
        "id": "zBQOInD-STEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        # Get the hyperparameters for this run\n",
        "        config = wandb.config\n",
        "        # # Initialize the environment and seed\n",
        "        # env = gym.make('Acrobot-v1')\n",
        "        # env.seed(0)\n",
        "\n",
        "        # # Get the state and action sizes for the Acrobot-v1 environment\n",
        "        # state_size = env.observation_space.shape[0]  # 6 dimensions\n",
        "        # action_size = env.action_space.n  # 3 actions\n",
        "\n",
        "        # Create the policy network with the specified hyperparameters\n",
        "        policy = Policy(state_size=state_shape, action_size=action_shape, hidden_size=config.hidden_size).to(device)\n",
        "        optimizer = optim.Adam(policy.parameters(), lr=config.lr)\n",
        "\n",
        "        # Create the baseline network with the specified hyperparameters\n",
        "        baseline = Baseline(state_size=state_shape).to(device)\n",
        "        baseline_optimizer = optim.Adam(baseline.parameters(), lr=config.baseline_lr)\n",
        "\n",
        "        # Run the REINFORCE algorithm with the specified hyperparameters\n",
        "        scores = reinforce(env, policy, optimizer, baseline, baseline_optimizer, n_episodes=config.n_episodes, max_t=config.max_t, gamma=config.gamma, print_every=config.print_every)\n",
        "\n",
        "        # # Check if the environment is solved\n",
        "        # if np.mean(scores[-100:]) >= -100:  # Adjust the threshold as needed\n",
        "        #     print(f'Environment {env.unwrapped.spec.id} solved in {e - 100:d} episodes!\\tAverage Score: {np.mean(scores_deque):.2f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Log the final score as a summary metric\n",
        "        run.summary[\"final_score\"] = np.mean(scores[-100:])\n",
        "\n",
        "        # # Finish the wandb run\n",
        "        # run.finish()\n",
        "\n",
        "        # # Return any necessary values or metrics\n",
        "        # return np.mean(scores[-100:])"
      ],
      "metadata": {
        "id": "ec7NSC4VcNhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, main, count=max_sweep_run)"
      ],
      "metadata": {
        "id": "2z4L975uOKw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}