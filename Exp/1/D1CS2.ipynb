{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Installing packages for rendering the game on Colab\n",
        "'''\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "zve95x_CZIk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34352f43-cea2-4d16-cdd8-0ff8387d23b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (69.2.0)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "# Replace with your actual API key\n",
        "api_key = \"17dab9d1bbdc37c41831799a4b0b50d3e97400c5\"\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=api_key)\n",
        "project_name = 'D1CS2'"
      ],
      "metadata": {
        "id": "LjcjlGuQZVfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "KtCihhmB8AnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp\n",
        "import wandb"
      ],
      "metadata": {
        "id": "2anBcjTiil7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8532683a-9e36-401d-a9be-6dd006219801"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "R5KV6KnciwII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206ef3b0-2ff1-405c-c8b5-cbb09394b5dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Dueling DQN model\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, fc1_units, fc2_units, seed):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(input_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.advantage = nn.Linear(fc2_units, output_size)\n",
        "        self.value = nn.Linear(fc2_units, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        advantage = self.advantage(x)\n",
        "        value = self.value(x)\n",
        "        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))\n",
        "        return q_values\n"
      ],
      "metadata": {
        "id": "5SXQlNoci9Ag"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ReplayBuffer class\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "BfzM7c3TjDxV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the agent\n",
        "class Agent():\n",
        "    def __init__(self, state_size, action_size, fc1_units, fc2_units, buffer_size, batch_size, lr, update_every, gamma, eps_end, eps_decay, seed):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.update_every = update_every\n",
        "        self.gamma = gamma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.q_network = DuelingDQN(state_size, action_size, fc1_units, fc2_units, seed).to(device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, fc1_units, fc2_units, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed)\n",
        "        self.t_step = 0\n",
        "        self.eps_end = eps_end\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) >= self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.0):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.q_network.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.q_network(state)\n",
        "        self.q_network.train()\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        Q_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # wandb.log({'train loss': loss})\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)  # Gradient clipping\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "9TzHGE1Ei_j1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn(agent, env, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    episode_list_epsgrdy = []\n",
        "    average_scores_epsgrdy = []\n",
        "    average_regret_epsgrdy = []\n",
        "    cumulative_regret_epsgrdy = []\n",
        "    cummulative_regret_window=[]\n",
        "    regret_window = []\n",
        "    cumulative_regret = 0  # Initialize cumulative regret\n",
        "    eps = eps_start\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        regret = 0  # Initialize regret for this episode\n",
        "        # cumulative_regret = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            optimal_action = np.argmax(agent.q_network(torch.from_numpy(state).float().unsqueeze(0)).cpu().data.numpy())\n",
        "            optimal_reward = env.step(optimal_action)[1]  # Get the reward for the optimal action\n",
        "            regret += optimal_reward - reward  # Calculate regret for this time step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "\n",
        "        scores_window.append(score)\n",
        "        average_score = np.mean(scores_window)\n",
        "        average_scores_epsgrdy.append(average_score)\n",
        "\n",
        "\n",
        "        regret_window.append(regret)\n",
        "        average_regret = np.mean(regret_window)\n",
        "        average_regret_epsgrdy.append(average_regret)\n",
        "\n",
        "        cumulative_regret += average_regret  # Update cumulative regret\n",
        "\n",
        "        # cummulative_regret_window.append(cumulative_regret)\n",
        "        # cummulative_average_regret = np.mean(cummulative_regret_window)\n",
        "        cummulative_average_regret = average_regret\n",
        "        cumulative_regret_epsgrdy.append(cumulative_regret)\n",
        "\n",
        "        episode_list_epsgrdy.append(i_episode)\n",
        "        wandb.log({'average_score': average_score})\n",
        "        wandb.log({'average_regret': average_regret})\n",
        "        wandb.log({\"cummulative_regret\": cumulative_regret})\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tRegret: {:.2f}'.format(i_episode, average_score, regret))\n",
        "\n",
        "        if np.mean(scores_window)>=195.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "\n",
        "    return episode_list_epsgrdy, average_scores_epsgrdy, average_regret_epsgrdy"
      ],
      "metadata": {
        "id": "oqeWaJOf7u4P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize wandb with your project name\n",
        "wandb.init(project=project_name)"
      ],
      "metadata": {
        "id": "EPQxglT2j1Kz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f91503ea-020e-4c9d-a84d-6cad6abd8192"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishi1906\u001b[0m (\u001b[33miitm_aero\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240409_015613-0iiy6oth</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/iitm_aero/D1CS2/runs/0iiy6oth' target=\"_blank\">fresh-snowflake-1</a></strong> to <a href='https://wandb.ai/iitm_aero/D1CS2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/iitm_aero/D1CS2' target=\"_blank\">https://wandb.ai/iitm_aero/D1CS2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/iitm_aero/D1CS2/runs/0iiy6oth' target=\"_blank\">https://wandb.ai/iitm_aero/D1CS2/runs/0iiy6oth</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/iitm_aero/D1CS2/runs/0iiy6oth?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c6f610431c0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOKV8a9CnAec",
        "outputId": "1893b3b9-8007-42d6-b0a1-4a8dcc89d6e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f78df0-4cfd-4922-f4b2-e3bb6af1cb1c",
        "id": "SluwY98d5OOy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: i7h94k9t\n",
            "Sweep URL: https://wandb.ai/iitm_aero/D1CS2/sweeps/i7h94k9t\n"
          ]
        }
      ],
      "source": [
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\"goal\": \"minimize\", \"name\": \"cumulative_regret\"},\n",
        "    \"parameters\": {\n",
        "        'state_size': {\n",
        "            'values': [state_shape]\n",
        "        },\n",
        "        'action_size': {\n",
        "            'values': [action_shape]\n",
        "        },\n",
        "        'BUFFER_SIZE': {\n",
        "            'values': [int(1e5)]\n",
        "            # 'values': [int(1e4), int(1e5), int(1e6)]\n",
        "        },\n",
        "        'BATCH_SIZE': {\n",
        "            'values': [64]\n",
        "            # 'values': [32, 64, 128, 256]\n",
        "        },\n",
        "        'LR': {\n",
        "            'values': [0.0001]\n",
        "            # 'values': [0.1, 0.01, 0.001, 0.0001]\n",
        "        },\n",
        "        'UPDATE_EVERY': {\n",
        "            'values': [20]\n",
        "            # 'values': [4, 6, 10, 20]\n",
        "        },\n",
        "        'fc1_units': {\n",
        "            # 'values': [128]\n",
        "            'values': [128]\n",
        "            # 'values': [64, 128, 256]\n",
        "        },\n",
        "        'fc2_units': {\n",
        "            # 'values': [64]\n",
        "            'values': [64]\n",
        "            # 'values': [64, 128, 256]\n",
        "        },\n",
        "        'eps_start': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'eps_end': {\n",
        "            'values': [0.01]\n",
        "            # 'values': [0.01, 0.05, 0.1]\n",
        "        },\n",
        "        'eps_decay': {\n",
        "            'values': [0.995]\n",
        "            # 'values': [0.9, 0.95, 0.99, 0.995, 0.999]\n",
        "        },\n",
        "        'gamma': {\n",
        "            'values': [0.99]\n",
        "        },\n",
        "        'n_episodes': {\n",
        "            'values': [5000]\n",
        "            # 'values': [1000, 2000, 5000]\n",
        "        },\n",
        "        'max_t': {\n",
        "            'values': [1000]\n",
        "            # 'values': [500, 1000, 2000]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_scores = []\n",
        "cumm_regret = []"
      ],
      "metadata": {
        "id": "nPSjuUNDAxim"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the main function for hyperparameter tuning\n",
        "def AvgOverExperiments():\n",
        "\n",
        "    # env = gym.make('Acrobot-v1')\n",
        "    # env.seed(0)\n",
        "    # state_shape = env.observation_space.shape[0]\n",
        "    # no_of_actions = env.action_space.n\n",
        "\n",
        "    with wandb.init() as run:\n",
        "        # Get the hyperparameters for this run\n",
        "        config = wandb.config\n",
        "\n",
        "        begin_time = datetime.datetime.now()\n",
        "        # env = gym.make('Acrobot-v1')\n",
        "        # env.seed(0)\n",
        "\n",
        "        # Create the agent with the hyperparameters\n",
        "        agent = Agent(state_size=config.state_size, action_size=config.action_size, fc1_units=config.fc1_units, fc2_units=config.fc2_units, buffer_size=config.BUFFER_SIZE, batch_size=config.BATCH_SIZE, lr=config.LR, update_every=config.UPDATE_EVERY, gamma=config.gamma, eps_end=config.eps_end, eps_decay=config.eps_decay,seed = 0)\n",
        "\n",
        "        # Train the agent and get the scores\n",
        "        # episode_list_epsgrdy, average_scores_epsgrdy = dqn(agent, env, n_episodes=config.n_episodes, max_t=config.max_t, eps_start=config.eps_start, eps_end=config.eps_end, eps_decay=config.eps_decay)\n",
        "\n",
        "        episode_list_epsgrdy, average_scores_epsgrdy, cumulative_regret_epsgrdy = dqn(agent, env, n_episodes=config.n_episodes, max_t=config.max_t, eps_start=config.eps_start, eps_end=config.eps_end, eps_decay=config.eps_decay)\n",
        "\n",
        "        time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "        print(time_taken)\n",
        "        # Log the final average score to wandb\n",
        "        # wandb.log({\"Average Score\": average_scores_epsgrdy})\n",
        "        # wandb.log({\"Average Score\": cumulative_regret})\n",
        "        print(average_scores_epsgrdy)\n",
        "        print('\\n')\n",
        "        print(cumulative_regret_epsgrdy)\n",
        "        average_scores.append(average_scores_epsgrdy)\n",
        "        cumm_regret.append(cumulative_regret_epsgrdy)\n",
        "        # return [average_scores_epsgrdy, cumulative_regret]\n"
      ],
      "metadata": {
        "id": "M4cke-mK2Pr0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_avg_curves(avg_reward,reward_std, avg_step, step_std, step_size = 1 ):\n",
        "\n",
        "    trunc_avg_reward = avg_reward[0::step_size]\n",
        "    tunc_reward_std = reward_std[0::step_size]\n",
        "\n",
        "    trunc_avg_step = avg_step[0::step_size]\n",
        "    tunc_step_std = step_std[0::step_size]\n",
        "\n",
        "    # Plot the average rewards with mean and standard deviation\n",
        "    plt.figure()\n",
        "    plt.plot(trunc_avg_reward, label='Mean')\n",
        "    plt.fill_between(range(step_size), trunc_avg_reward + tunc_reward_std, trunc_avg_reward - tunc_reward_std, alpha=0.5, label=' Std Dev')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Total Reward')\n",
        "    plt.title('Average Total Reward vs Episode')\n",
        "    plt.legend()\n",
        "    # plt.savefig( \"RvsE \" +\".pdf\")\n",
        "    # files.download(exp_no+\"_RvsE.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Plot the average rewards with mean and standard deviation\n",
        "    plt.figure()\n",
        "    plt.plot(trunc_avg_step, label='Mean')\n",
        "    plt.fill_between(range(step_size), trunc_avg_step + tunc_step_std, trunc_avg_step - tunc_step_std, alpha=0.5, label=' Std Dev')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Average Regret')\n",
        "    plt.title(' Average Regret vs Episode')\n",
        "    plt.legend()\n",
        "    # plt.savefig(\"SvsE \" + \".pdf\")\n",
        "    # files.download(exp_no+\"_SvsE.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the average rewards with mean and standard deviation\n",
        "    plt.figure()\n",
        "    plt.plot(avg_step, label='Mean')\n",
        "    # plt.fill_between(range(step_size), trunc_avg_step + tunc_step_std, trunc_avg_step - tunc_step_std, alpha=0.5, label=' Std Dev')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Cummulative Regret')\n",
        "    plt.title('Cummulative Average Regret vs Episode')\n",
        "    plt.legend()\n",
        "    # plt.savefig(\"SvsE \" + \".pdf\")\n",
        "    # files.download(exp_no+\"_SvsE.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bKMK9VnNK_-C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Average over 5 exeriments\n",
        "\n",
        "# average_scores, cumm_regret = [], []\n",
        "num_expts = 5\n",
        "for i in range(num_expts):\n",
        "    print(\"Experiment: %d\" % (i + 1))\n",
        "    # episode_average_scores, episode_cumm_regret =  wandb.agent(sweep_id, function=AvgOverExperiments, count=1)\n",
        "    wandb.agent(sweep_id, function=AvgOverExperiments, count=1)\n",
        "    # average_scores.append(episode_average_scores)\n",
        "    # cumm_regret.append(episode_cumm_regret)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "7XVhKNa22FaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_scores = np.array(average_scores)\n",
        "cumm_regret = np.array(cumm_regret)\n",
        "\n",
        "mean_average_scores = np.mean(average_scores, axis=0)\n",
        "std_mean_average_scores = np.std(average_scores, axis=0)\n",
        "\n",
        "mean_cumm_regret = np.mean(cumm_regret, axis=0)\n",
        "mean_cumm_regret = np.std(cumm_regret, axis=0)\n",
        "\n",
        "plot_avg_curves(mean_average_scores, std_mean_average_scores, mean_cumm_regret, mean_cumm_regret, step_size = 10)"
      ],
      "metadata": {
        "id": "zTKqtKWa4B9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}