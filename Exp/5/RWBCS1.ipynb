{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "# Replace with your actual API key\n",
        "api_key = \"17dab9d1bbdc37c41831799a4b0b50d3e97400c5\"\n",
        "project_name='RWBCS1'\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlW33Sjz2WUd",
        "outputId": "f7392131-8a1b-4fdd-e815-0f85a5d3a3fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.44.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrishi1906\u001b[0m (\u001b[33miitm_aero\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "torch.manual_seed(0)\n",
        "import base64\n",
        "import io  # For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "import datetime"
      ],
      "metadata": {
        "id": "r9eE0OjXWWW0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJTy0fUIWY_r",
        "outputId": "5d026c6a-edfb-4248-8562-0aca5f2d92fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "# print('observation space:', env.observation_space)\n",
        "# print('action space:', env.action_space)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hse5VDflWeUk",
        "outputId": "5e12bb50-bfc4-4365-d67e-3b401ea5e5d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        # we just consider 1 dimensional probability of action\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        model = Categorical(probs)\n",
        "        action = model.sample()\n",
        "        return action.item(), model.log_prob(action)\n"
      ],
      "metadata": {
        "id": "Z2ON5IlyXLqc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T1B1nA1J0Fxa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reinforce(env, policy, optimizer, n_episodes=2000, max_t=1000, gamma=0.99, print_every=100):\n",
        "    scores_window = deque(maxlen=100)\n",
        "    scores = []\n",
        "    average_scores_epsgrdy = []\n",
        "    episode_list_epsgrdy = []\n",
        "    for e in range(1, n_episodes):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        # Collect trajectory\n",
        "        for t in range(max_t):\n",
        "            # Sample the action from current policy\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        # Calculate total expected reward\n",
        "        scores_window.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        average_score = np.mean(scores_window)\n",
        "        average_scores_epsgrdy.append(average_score)\n",
        "\n",
        "        wandb.log({'average_score': average_score})\n",
        "\n",
        "        episode_list_epsgrdy.append(e)\n",
        "\n",
        "\n",
        "        # Recalculate the total reward applying discounted factor\n",
        "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
        "        R = sum([a * b for a,b in zip(discounts, rewards)])\n",
        "\n",
        "        # Calculate the loss\n",
        "        policy_loss = []\n",
        "        for t, log_prob in enumerate(saved_log_probs):\n",
        "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
        "            policy_loss.append(-log_prob * R )\n",
        "        # After that, we concatenate whole policy loss in 0th dimension\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)))\n",
        "        if np.mean(scores_window) >= 195.0:\n",
        "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_window)))\n",
        "            break\n",
        "    return episode_list_epsgrdy, average_scores_epsgrdy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=project_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "lI9JpNhurdWD",
        "outputId": "ddd05b77-2ee6-4230-cb00-0cca8828e5b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240408_221559-r7qjg2ku</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/iitm_aero/RWBCS1/runs/r7qjg2ku' target=\"_blank\">avid-durian-58</a></strong> to <a href='https://wandb.ai/iitm_aero/RWBCS1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/iitm_aero/RWBCS1' target=\"_blank\">https://wandb.ai/iitm_aero/RWBCS1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/iitm_aero/RWBCS1/runs/r7qjg2ku' target=\"_blank\">https://wandb.ai/iitm_aero/RWBCS1/runs/r7qjg2ku</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/iitm_aero/RWBCS1/runs/r7qjg2ku?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7cd7c966e200>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "      'name': 'Average Reward',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'state_size': {\n",
        "            'values': [state_shape]\n",
        "        },\n",
        "        'action_size': {\n",
        "            'values': [action_shape]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [32,64,128]\n",
        "        },\n",
        "        'max_t': {\n",
        "            'values': [500, 1000, 1500]\n",
        "        },\n",
        "        'lr': {\n",
        "            'values': [1e-2, 1e-3,1e-4]\n",
        "        },\n",
        "        'gamma': {\n",
        "            'values': [0.99]\n",
        "        },\n",
        "        'alpha': {\n",
        "            'values': [0.01, 0.001, 0.0001]\n",
        "        },\n",
        "        'n_episodes': {\n",
        "            'values': [5000]\n",
        "        },\n",
        "        \"max_t\":{\n",
        "            'values': [1000]\n",
        "        },\n",
        "         \"print_every\": {\n",
        "             'values': [100]\n",
        "         }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Sn8tdcXc1Gte"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new sweep\n",
        "sweep_id = wandb.sweep(sweep_config,  project=project_name)\n",
        "max_sweep_run = 10 #update it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QMZhG_F0TZB",
        "outputId": "f1c2ec91-e6d6-40ba-ebaf-c9c7d11d99c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: ud26lmsz\n",
            "Sweep URL: https://wandb.ai/iitm_aero/RWBCS1/sweeps/ud26lmsz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with wandb.init() as run:\n",
        "        # Get the hyperparameters for this run\n",
        "        config = wandb.config\n",
        "\n",
        "        # begin_time = datetime.datetime.now()\n",
        "\n",
        "\n",
        "        # Create the policy network with the specified hyperparameters\n",
        "        policy = Policy(state_size=4, action_size=2, hidden_size=config.hidden_size).to(device)\n",
        "        optimizer = optim.Adam(policy.parameters(), lr=config.lr)\n",
        "\n",
        "        # Run the REINFORCE algorithm with the specified hyperparameters\n",
        "        episode_list_epsgrdy, average_scores_epsgrdy = reinforce(policy, optimizer, n_episodes=config.n_episodes, max_t=config.max_t, gamma=config.gamma, print_every=config.print_every)\n",
        "\n",
        "        # time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "        # print(time_taken)\n",
        "        # Log the final average score to wandb\n",
        "        # wandb.log({\"Average Score\": average_scores_epsgrdy})\n",
        "        # wandb.log({\"Average Score\": cumulative_regret})\n",
        "\n"
      ],
      "metadata": {
        "id": "p9qbRfNIsXmV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wandb.agent(sweep_id, main, count=max_sweep_run)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "47lgJGSB0cp8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}